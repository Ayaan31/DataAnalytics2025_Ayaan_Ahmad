# ength box plot
ggplot(data = dat) +
geom_boxplot(aes(y = length, yend = length), size = 0.3, color = "red") +
theme_classic()+
theme(axis.title.y = element_text(color = "orange")) +
ylab("Length")
# Above 0.8 = sufficient power
# Below 0.8 = not sufficient power
wp.correlation(n = 50, r = 0.3, alpha = 0.05)
library(WebPower)
# Above 0.8 = sufficient power
# Below 0.8 = not sufficient power
wp.correlation(n = 50, r = 0.3, alpha = 0.05)
# TO get it above 0.8, we can change r and/or sample size. Do not recommend changing alpha
wp.correlation(n = 50, r = 0.384, alpha = 0.05)
# TO get it above 0.8, we can change r and/or sample size. Do not recommend changing alpha
wp.correlation(n = 50, r = 0.384, alpha = 0.1)
# TO get it above 0.8, we can change r and/or sample size. Do not recommend changing alpha
wp.correlation(n = 50, r = 0.384, alpha = 0.1)
# TO get it above 0.8, we can change r and/or sample size. Do not recommend changing alpha
wp.correlation(n = 50, r = 0.384, alpha = 0.001)
# TO get it above 0.8, we can change r and/or sample size. Do not recommend changing alpha
wp.correlation(n = 50, r = 0.384, alpha = 0.05)
# TO get it above 0.8, we can change r and/or sample size. Do not recommend changing alpha
wp.correlation(n = 50, r = 0.384, alpha = 0.5)
0
# TO get it above 0.8, we can change r and/or sample size. Do not recommend changing alpha
wp.correlation(n = 50, r = 0.384, alpha = 0.05)
# Fixed contraints, can only afford 50 samples
wp.correlation(n = 50, alpha = 0.05, power = 0.8)
wp.correlation(r = 0.3, alpha = 0.05, power = 0.8)
# plot sample size power
example <- wp.correlation(n=seq(50, 100, 10),
0.3, alpha = 0.05)
plot(example)
# D = effect size, how many stds away
wp.t(n1 = NULL, d = 0.03, power = 0.08, type = "paired")
wp.t(n1 = NULL, d = 0.03, power = 0.08, type = "two.sample")
# We can sample 1000
# Dont know effect size
# What effect size can we detect
wp.t(n1 = 1000, d = NULL, power = 0.8, type = "two.sample")
# .2n is for when we have two sample sizes
wp.t(n1 = 100, n2 = 250, d = 0.5, power = NULL, type = "two.sample.2n")
library(tidyverse)
library(caret)
epi_data <- read.csv("epi_results_2024_pop_gdp_v2.csv")
setwd("C:/Users/ahmad/Desktop/OneDrive - Rensselaer Polytechnic Institute/RPI(F25)/DataAnalytics2025_Ayaan_Ahmad/Assignments/Assignment2")
epi_data <- read.csv("epi_results_2024_pop_gdp_v2.csv")
epi_east_europe <- subset(epi_data, region == "Eastern Europe")
epi_south_asia <- subset(epi_data, region == "Southern Asia")
# Part 3: Classification with kNN
knn_data <- epi_data %>% select(EPI.new, ECO.new, BDH.new, region)
train_indices <- createDataPartition(knn_data$region, p = 0.7, list = FALSE)
train_data <- knn_data[train_indices, ]
test_data <- knn_data[-train_indices, ]
ctrl <- trainControl(method = "cv", number = 10)
# Train the kNN model
k_values <- c(3, 5, 7, 9, 11)
knn_model <- train(
region ~ .,
data = train_data,
method = "knn",
trControl = ctrl,
tuneGrid = expand.grid(k = k_values)
)
knn_predictions <- predict(knn_model, newdata = test_data)
confusion_matrix <- confusionMatrix(knn_predictions, test_data$region)
knn_predictions <- predict(knn_model, newdata = test_data)
confusion_matrix <- confusionMatrix(knn_predictions, test_data$region)
# Part 3: Classification with kNN
knn_data <- epi_data %>% select(EPI.new, ECO.new, BDH.new, region)
train_indices <- createDataPartition(knn_data$region, p = 0.7, list = FALSE)
train_data <- knn_data[train_indices, ] # 70% training, 30% test data
test_data <- knn_data[-train_indices, ]
ctrl <- trainControl(method = "cv", number = 10)
# Train the kNN model
k_values <- c(3, 5, 7)
knn_model <- train(
region ~ .,
data = train_data,
method = "knn",
trControl = ctrl,
tuneGrid = expand.grid(k = k_values)
)
knn_predictions <- predict(knn_model, newdata = test_data)
levels(knn_predictions) <- levels(test_data$region)
ctrl <- trainControl(method = "cv", number = 10)
# Train the kNN model
knn_model <- train(
region~.,
data = train_data,
method = "knn",
)
knn_predictions <- predict(knn_model, newdata = test_data)
levels(knn_predictions) <- levels(test_data$region)
knn.train.predicted <- predict(knn_model,train_data[,-5])
knn_test_predictions <- predict(knn_model,test_data[,-5])
train_confusion_matrix = as.matrix(table(Actual = knn_train_predicted, Predicted = knn_test_predictions))
knn_train_predicted <- predict(knn_model,train_data[,-5])
knn_test_predictions <- predict(knn_model,test_data[,-5])
train_confusion_matrix = as.matrix(table(Actual = knn_train_predicted, Predicted = knn_test_predictions))
ctrl <- trainControl(method = "cv", number = 10)
# Train the kNN model
knn_model <- train(
region~.,
data = train_data,
method = "knn",
)
knn_train_true <- train_data[,5]
knn_train_predicted <- predict(knn_model,train_data[,-5])
knn_test_predicted <- predict(knn_model,test_data[,-5])
train_confusion_matrix = as.matrix(table(Actual = knn_train_true, Predicted = knn_test_predictions))
knn_train_true <- train_data[,5]
# Part 3: Classification with kNN
knn_data <- epi_data %>% select(EPI.new, ECO.new, BDH.new, region)
knn_data$region <- as.factor(knn_data$region)
View(knn_data)
View(knn_data)
train_indices <- createDataPartition(knn_data$region, p = 0.7, list = FALSE)
train_data <- knn_data[train_indices, ] # 70% training, 30% test data
test_data <- knn_data[-train_indices, ]
# Train the kNN model
knn_model <- train(
region~.,
data = train_data,
method = "knn",
)
knn_test_predictions <- predict(knn_model, newdata = test_data)
# 2. Get the ACTUAL labels from the TEST data to compare against.
knn_test_true <- test_data$region
# 3. Use caret's confusionMatrix() function to evaluate the model
# This compares the predicted values against the true values for the test set.
confusion_matrix <- confusionMatrix(data = knn_test_predictions, reference = knn_test_true)
# Print the full confusion matrix and its statistics
print(confusion_matrix)
# You can also print just the accuracy if you want
cat("Accuracy of correct classifications:", confusion_matrix$overall['Accuracy'], "\n")
train_accuracy <- sum(diag(confusion_matrix))/nrow(train_data)
# You can also print just the accuracy if you want
print("Accuracy of correct classifications:", confusion_matrix$overall['Accuracy'], "\n")
knn_test_predictions <- predict(knn_model, newdata = test_data)
# 2. Get the ACTUAL labels from the TEST data to compare against.
knn_test_true <- test_data$region
# Built in confusion matrix instead of doing as.Matrix() shown in example code
confusion_matrix <- confusionMatrix(data = knn_test_predictions, reference = knn_test_true)
# Print the full confusion matrix and its statistics
print(confusion_matrix)
# You can also print just the accuracy if you want
print("Accuracy of correct classifications:", confusion_matrix$overall['Accuracy'], "\n")
cat("Accuracy of correct classifications:", confusion_matrix$overall['Accuracy'], "\n")
# Print the full confusion matrix and its statistics
confusion_matrix
# Part 3: Classification with kNN
knn_data <- epi_data %>% select(EPI.new, ECO.new, BDH.new, region)
knn_data$region <- as.factor(knn_data$region)
train_indices <- createDataPartition(knn_data$region, p = 0.7, list = FALSE)
train_data <- knn_data[train_indices, ] # 70% training, 30% test data
test_data <- knn_data[-train_indices, ]
k_value <- data.frame(k = 5)
# Train the kNN model
knn_model <- train(
region~.,
data = train_data,
method = "knn",
tuneGrid = k_value
)
knn_test_predictions <- predict(knn_model, newdata = test_data)
# 2. Get the ACTUAL labels from the TEST data to compare against.
knn_test_true <- test_data$region
# Built in confusion matrix instead of doing as.Matrix() shown in example code
confusion_matrix <- confusionMatrix(data = knn_test_predictions, reference = knn_test_true)
confusion_matrix
# kNN model with 3 other variables
knn_data_2 <- epi_data %>% select(PAR.new, PHL.new, APO.new, region)
knn_data_2$region <- as.factor(knn_data_2$region)
train_indices <- createDataPartition(knn_data_2$region, p = 0.7, list = FALSE)
train_data <- knn_data_2[train_indices, ] # 70% training, 30% test data
test_data <- knn_data_2[-train_indices, ]
k_value <- data.frame(k = 5)
# Train the kNN model
knn_model <- train(
region~.,
data = train_data,
method = "knn",
tuneGrid = k_value
)
# kNN model with 3 other variables
knn_data <- epi_data %>% select(PAR.new, PHL.new, APO.new, region)
knn_data$region <- as.factor(knn_data$region)
train_indices <- createDataPartition(knn_data$region, p = 0.7, list = FALSE)
train_data <- knn_data[train_indices, ] # 70% training, 30% test data
test_data <- knn_data[-train_indices, ]
k_value <- data.frame(k = 5)
# Train the kNN model
knn_model <- train(
region~.,
data = train_data,
method = "knn",
tuneGrid = k_value
)
# kNN model with 3 other variables
knn_data <- epi_data %>% select(PAR.new, PHL.new, APO.new, region) %>% drop_na()
knn_data$region <- as.factor(knn_data$region)
train_indices <- createDataPartition(knn_data$region, p = 0.7, list = FALSE)
train_data <- knn_data[train_indices, ] # 70% training, 30% test data
test_data <- knn_data[-train_indices, ]
k_value <- data.frame(k = 5)
# Train the kNN model
knn_model <- train(
region~.,
data = train_data,
method = "knn",
tuneGrid = k_value
)
knn_test_predictions <- predict(knn_model, newdata = test_data)
# Get the labels from the test data to compare against.
knn_test_true <- test_data$region
# Built in confusion matrix instead of doing as.Matrix() shown in example code
confusion_matrix <- confusionMatrix(data = knn_test_predictions, reference = knn_test_true)
confusion_matrix
cat("Accuracy of correct classifications:", confusion_matrix$overall['Accuracy'], "\n")
# Part 3: Classification with kNN
knn_data <- epi_data %>% select(EPI.new, ECO.new, BDH.new, region)
knn_data$region <- as.factor(knn_data$region)
train_indices <- createDataPartition(knn_data$region, p = 0.7, list = FALSE)
train_data <- knn_data[train_indices, ] # 70% training, 30% test data
test_data <- knn_data[-train_indices, ]
k_value <- data.frame(k = 5)
# Train the kNN model
knn_model <- train(
region~.,
data = train_data,
method = "knn",
tuneGrid = k_value
)
knn_test_predictions <- predict(knn_model, newdata = test_data)
# Get the labels from the testdata to compare against.
knn_test_true <- test_data$region
# Built in confusion matrix instead of doing as.Matrix() shown in example code
confusion_matrix <- confusionMatrix(data = knn_test_predictions, reference = knn_test_true)
confusion_matrix
cat("Accuracy of correct classifications:", confusion_matrix$overall['Accuracy'], "\n")
# SECTION 3.2 kNN model with 3 other variables
knn_data <- epi_data %>% select(PAR.new, PHL.new, APO.new, region) %>% drop_na()
knn_data$region <- as.factor(knn_data$region)
# SECTION 3.2 kNN model with 3 other variables
knn_data <- epi_data %>% select(PAR.new, PHL.new, APO.new, region) %>% drop_na()
knn_data$region <- as.factor(knn_data$region)
train_indices <- createDataPartition(knn_data$region, p = 0.7, list = FALSE)
train_data <- knn_data[train_indices, ] # 70% training, 30% test data
test_data <- knn_data[-train_indices, ]
k_value <- data.frame(k = 5)
# Train the kNN model
knn_model <- train(
region~.,
data = train_data,
method = "knn",
tuneGrid = k_value
)
knn_test_predictions <- predict(knn_model, newdata = test_data)
# Get the labels from the test data to compare against.
knn_test_true <- test_data$region
# Built in confusion matrix instead of doing as.Matrix() shown in example code
confusion_matrix <- confusionMatrix(data = knn_test_predictions, reference = knn_test_true)
confusion_matrix
cat("Accuracy of correct classifications:", confusion_matrix$overall['Accuracy'], "\n")
