# ength box plot
ggplot(data = dat) +
geom_boxplot(aes(y = length, yend = length), size = 0.3, color = "red") +
theme_classic()+
theme(axis.title.y = element_text(color = "orange")) +
ylab("Length")
# Above 0.8 = sufficient power
# Below 0.8 = not sufficient power
wp.correlation(n = 50, r = 0.3, alpha = 0.05)
library(WebPower)
# Above 0.8 = sufficient power
# Below 0.8 = not sufficient power
wp.correlation(n = 50, r = 0.3, alpha = 0.05)
# TO get it above 0.8, we can change r and/or sample size. Do not recommend changing alpha
wp.correlation(n = 50, r = 0.384, alpha = 0.05)
# TO get it above 0.8, we can change r and/or sample size. Do not recommend changing alpha
wp.correlation(n = 50, r = 0.384, alpha = 0.1)
# TO get it above 0.8, we can change r and/or sample size. Do not recommend changing alpha
wp.correlation(n = 50, r = 0.384, alpha = 0.1)
# TO get it above 0.8, we can change r and/or sample size. Do not recommend changing alpha
wp.correlation(n = 50, r = 0.384, alpha = 0.001)
# TO get it above 0.8, we can change r and/or sample size. Do not recommend changing alpha
wp.correlation(n = 50, r = 0.384, alpha = 0.05)
# TO get it above 0.8, we can change r and/or sample size. Do not recommend changing alpha
wp.correlation(n = 50, r = 0.384, alpha = 0.5)
0
# TO get it above 0.8, we can change r and/or sample size. Do not recommend changing alpha
wp.correlation(n = 50, r = 0.384, alpha = 0.05)
# Fixed contraints, can only afford 50 samples
wp.correlation(n = 50, alpha = 0.05, power = 0.8)
wp.correlation(r = 0.3, alpha = 0.05, power = 0.8)
# plot sample size power
example <- wp.correlation(n=seq(50, 100, 10),
0.3, alpha = 0.05)
plot(example)
# D = effect size, how many stds away
wp.t(n1 = NULL, d = 0.03, power = 0.08, type = "paired")
wp.t(n1 = NULL, d = 0.03, power = 0.08, type = "two.sample")
# We can sample 1000
# Dont know effect size
# What effect size can we detect
wp.t(n1 = 1000, d = NULL, power = 0.8, type = "two.sample")
# .2n is for when we have two sample sizes
wp.t(n1 = 100, n2 = 250, d = 0.5, power = NULL, type = "two.sample.2n")
setwd("C:/Users/ahmad/Desktop/OneDrive - Rensselaer Polytechnic Institute/RPI(F25)/DataAnalytics2025_Ayaan_Ahmad/Lab4")
## read dataset
wine <- read_csv("wine.data", col_names = FALSE)
## load libraries
library(ggplot2)
library(ggfortify)
library(GGally)
library(e1071)
## load libraries
library(ggplot2)
library(ggfortify)
library(GGally)
library(e1071)
library(class)
library(psych)
library(readr)
## read dataset
wine <- read_csv("wine.data", col_names = FALSE)
View(wine)
## set column names
names(wine) <- c("Type","Alcohol","Malic acid","Ash","Alcalinity of ash","Magnesium","Total phenols","Flavanoids","Nonflavanoid Phenols","Proanthocyanins","Color Intensity","Hue","Od280/od315 of diluted wines","Proline")
View(wine)
## inspect data frame
head(wine)
wine$Type <- as.factor(wine$Type)
## visualize variables
pairs.panels(wine[,-1],gap = 0,bg = c("red", "yellow", "blue")[wine$Type],pch=21)
ggpairs(wine, ggplot2::aes(colour = Type))
## visualize variables
pairs.panels(wine[,-1],gap = 0,bg = c("red", "yellow", "blue")[wine$Type],pch=21)
where(iris)
###
scaled_wine <- scale(wine)
## read dataset
wine <- read_csv("wine.data", col_names = FALSE)
## set column names
names(wine) <- c("Type","Alcohol","Malic acid","Ash","Alcalinity of ash","Magnesium","Total phenols","Flavanoids","Nonflavanoid Phenols","Proanthocyanins","Color Intensity","Hue","Od280/od315 of diluted wines","Proline")
## inspect data frame
head(wine)
wine$Type <- as.factor(wine$Type)
## visualize variables
pairs.panels(wine[,-1],gap = 0,bg = c("red", "yellow", "blue")[wine$Type],pch=21)
ggpairs(wine, ggplot2::aes(colour = Type))
###
# The first column is the "Type" which is the label
# We want to grab every column that is not this first one
wine_features = wine[, -1]
# Put "Type" column in this data frame
wine_labels = wine[, 1]
wine_pca <- princomp(wine, cor = TRUE)
wine_pca <- princomp(wine_features, cor = TRUE)
summary(wine_pca)
pca_sumarry <- summary(wine_pca)
pca_sumarry
wine_pca <- princomp(wine_features, cor = TRUE)
pca_summary <- summary(wine_pca)
pca_summary
wine_pca$loadings
ggplot(wine_pca$scores, aes(x = Comp.1, y = Comp.2)) +
geom_point(color = "blue")
ggplot(wine_pca$scores, aes(x = Comp.1, y = Comp.2, color = Type)) +
geom_point()
ggplot(wine_pca$scores, aes(x = Comp.1, y = Comp.2)) +
geom_point(color = Type)
ggplot(wine_pca$scores, aes(x = Comp.1, y = Comp.2)) +
geom_point(color = wine$Type)
ggplot(wine_pca$scores, aes(x = Comp.1, y = Comp.2)) +
geom_point(color = wine$Type)
ggplot(wine_pca$scores, aes(x = Comp.1, y = Comp.2, color = wine$Type)) +
geom_point(color = wine$Type)
ggplot(wine_pca$scores, aes(x = Comp.1, y = Comp.2, color = wine$Type)) +
geom_point(size = 3)
ggplot(wine_pca$scores, aes(x = Comp.1, y = Comp.2, color = wine$Type)) +
geom_point(size = 3) +
labs(
title = "First Two Pricipal Components of Wine Dataset",
x = "PC 1",
y = "PC 2",
color = "Wine Type"
) + theme_bw()
wine_pca$loadings
# Top contributors for PC1 (Comp.1)
pc1_loadings_sorted <- loadings[order(abs(loadings[, 1]), decreasing = TRUE), 1]
# Top contributors for PC1 (Comp.1)
pc1_loadings_sorted <- loadings[order(abs(loadings[, 1]), decreasing = TRUE), 1]
loadings <- wine_pca$loadings
# Top contributors for PC1 (Comp.1)
pc1_loadings_sorted <- loadings[order(abs(loadings[, 1]), decreasing = TRUE), 1]
print("Top Contributors to PC1 (by absolute value)")
print(pc1_loadings_sorted)
# Top contributors for PC2 (Comp.2)
pc2_loadings_sorted <- loadings[order(abs(loadings[, 2]), decreasing = TRUE), 2]
print("\ Top Contributors to PC2 (by absolute value)")
print(pc2_loadings_sorted)
train_indices <- createDataPartition(wine$Type, p = 0.7, list = FALSE)
library(caret)
train_indices <- createDataPartition(wine$Type, p = 0.7, list = FALSE)
# Original dataset split
train_set_orig <- wine[train_indices, ]
test_set_orig <- wine[-train_indices, ]
# Store the "ground truth" labels from the test set for comparison
test_labels_actual <- test_set_orig$Type
# 1. Run princomp on the *full* feature set
wine_pca_obj <- princomp(wine[, -1], cor = TRUE)
# 2. Create a new dataframe with ONLY PC scores and the Type
pca_data <- data.frame(
Type = wine$Type,
wine_pca$scores
)
# 3. Split the PCA data using the *same indices* for a fair comparison
train_set_pca <- pca_data[train_indices, ]
test_set_pca <- pca_data[-train_indices, ]
cat("--- Training Model 1 (All 13 Original Features) ---\n\n")
# We use 10-fold cross-validation (cv) to find the best 'k'
# preProcess = c("center", "scale") automatically standardizes the data
# This is required for kNN, which is distance-based.
ctrl <- trainControl(method = "cv", number = 10)
model_orig <- train(
Type ~ .,  # Formula: "Predict Type using all other variables"
data = train_set_orig,
method = "knn",
trControl = ctrl,
preProcess = c("center", "scale"),
tuneLength = 10  # Automatically tests 10 different 'k' values
)
# Show the best k that was chosen
print(model_orig)
cat("\n--- Training Model 2 (Only PC1 and PC2) ---\n\n")
model_pca <- train(
Type ~ PC1 + PC2,  # Formula: "Predict Type using only PC1 and PC2"
data = train_set_pca,
method = "knn",
trControl = ctrl,
# No preProcess needed; PCA data is already centered/scaled
tuneLength = 10
)
model_pca <- train(
Type ~ Comp.1 + Comp.2,  # Formula: "Predict Type using only PC1 and PC2"
data = train_set_pca,
method = "knn",
trControl = ctrl,
# No preProcess needed; PCA data is already centered/scaled
tuneLength = 10
)
# Show the best k that was chosen
print(model_pca)
# Get predictions on the test set
pred_orig <- predict(model_orig, newdata = test_set_orig)
pred_pca <- predict(model_pca, newdata = test_set_pca)
# Generate the full comparison report
cm_orig <- confusionMatrix(data = pred_orig, reference = test_labels_actual)
cm_pca <- confusionMatrix(data = pred_pca, reference = test_labels_actual)
# Print the results
cat("\n\n--- Comparison Report ---\n")
cat("===================================================\n")
cat("      Model 1: All 13 Original Features\n")
cat("===================================================\n")
print(cm_orig)
cat("\n===================================================\n")
cat("      Model 2: Only PC1 & PC2\n")
cat("===================================================\n")
print(cm_pca)
# Generate the full comparison report
cm_orig <- confusionMatrix(data = pred_orig, reference = test_labels_actual)
cm_pca <- confusionMatrix(data = pred_pca, reference = test_labels_actual)
# Print the results
print("Orginal Dataset Model Performance:")
print(cm_orig)
print("PCA Dataset Model Performance:")
print(cm_pca)
train_indices <- createDataPartition(wine$Type, p = 0.7, list = FALSE)
# Original dataset split
train_set_orig <- wine[train_indices, ]
test_set_orig <- wine[-train_indices, ]
# Store the "ground truth" labels from the test set for comparison
test_labels_actual <- test_set_orig$Type
# Create a new dataframe with ONLY PC scores and the Type
pca_data <- data.frame(
Type = wine$Type,
wine_pca$scores
)
# Split the PCA data using the same indices for a fair comparison
train_set_pca <- pca_data[train_indices, ]
test_set_pca <- pca_data[-train_indices, ]
ctrl <- trainControl(method = "cv", number = 10)
model_orig <- train(
Type ~ .,  # Formula: "Predict Type using all other variables"
data = train_set_orig,
method = "knn",
trControl = ctrl,
preProcess = c("center", "scale"),
tuneLength = 10  # Automatically tests 10 different 'k' values
)
# Show the best k that was chosen
print(model_orig)
model_pca <- train(
Type ~ Comp.1 + Comp.2,  # Formula: "Predict Type using only PC1 and PC2"
data = train_set_pca,
method = "knn",
trControl = ctrl,
tuneLength = 10
)
# Show the best k that was chosen
print(model_pca)
# Get predictions on the test set
pred_orig <- predict(model_orig, newdata = test_set_orig)
pred_pca <- predict(model_pca, newdata = test_set_pca)
# Generate the full comparison report
cm_orig <- confusionMatrix(data = pred_orig, reference = test_labels_actual)
cm_pca <- confusionMatrix(data = pred_pca, reference = test_labels_actual)
# Print the results
print("Orginal Dataset Model Performance:")
print(cm_orig)
print("PCA Dataset Model Performance:")
print(cm_pca)
train_indices <- createDataPartition(wine$Type, p = 0.7, list = FALSE)
# Original dataset split
train_set_orig <- wine[train_indices, ]
test_set_orig <- wine[-train_indices, ]
# Store the "ground truth" labels from the test set for comparison
test_labels_actual <- test_set_orig$Type
# Create a new dataframe with ONLY PC scores and the Type
pca_data <- data.frame(
Type = wine$Type,
wine_pca$scores
)
# Split the PCA data using the same indices for a fair comparison
train_set_pca <- pca_data[train_indices, ]
test_set_pca <- pca_data[-train_indices, ]
ctrl <- trainControl(method = "cv", number = 10)
model_orig <- train(
Type ~ .,  # Formula: "Predict Type using all other variables"
data = train_set_orig,
method = "knn",
trControl = ctrl,
preProcess = c("center", "scale"),
tuneLength = 10  # Automatically tests 10 different 'k' values
)
# Show the best k that was chosen
print(model_orig)
model_pca <- train(
Type ~ Comp.1 + Comp.2,  # Formula: "Predict Type using only PC1 and PC2"
data = train_set_pca,
method = "knn",
trControl = ctrl,
preProcess = c("center", "scale"),
tuneLength = 10
)
# Show the best k that was chosen
print(model_pca)
# Get predictions on the test set
pred_orig <- predict(model_orig, newdata = test_set_orig)
pred_pca <- predict(model_pca, newdata = test_set_pca)
# Generate the full comparison report
cm_orig <- confusionMatrix(data = pred_orig, reference = test_labels_actual)
cm_pca <- confusionMatrix(data = pred_pca, reference = test_labels_actual)
# Print the results
print("Orginal Dataset Model Performance:")
print(cm_orig)
print("PCA Dataset Model Performance:")
print(cm_pca)
